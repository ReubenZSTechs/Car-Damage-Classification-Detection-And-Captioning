{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ed4a8b",
   "metadata": {},
   "source": [
    "**<h1>Building the final model</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6d0c4",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0cb4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reube\\anaconda3\\envs\\Portfolio_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from ultralytics import YOLO\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, GPT2Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f0e42",
   "metadata": {},
   "source": [
    "# Define the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58cc286",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'YOLO_LOCATION_PATH': '../../models/final_model/damage_location_model/yolov8_damage_location_best.pt',\n",
    "    'YOLO_SEVERITY_PATH': '../../models/final_model/damage_severity_model/yolov8_damage_severity_best.pt',\n",
    "    'RESNET_CLASSIFIER_PATH': '../../models/final_model/classification_model/resnet50_classifier.pth',\n",
    "    'CAPTIONING_MODEL': '../../models/final_model/caption_model',\n",
    "    'SEVERITY_NAMES': ['low', 'medium', 'high'],\n",
    "    'LOCATION_NAMES': ['front', 'back', 'rear-left', 'rear-right'],\n",
    "    'TEST_DIR': '../../data/random_images',\n",
    "    'MODEL_FINAL_SAVE_DIR': '../../models/inference_model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39a0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f1394",
   "metadata": {},
   "source": [
    "# Define the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31780b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet50CustomModel(torch.nn.Module):\n",
    "    def __init__(self, dropout1=0.3, dropout2=0.4, dropout3=0.5, out1=1024, out2=512, out3=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.extractor = models.resnet50(pretrained=True)\n",
    "        in_features = self.extractor.fc.in_features # Extract only the vector feature importance\n",
    "        self.extractor.fc = torch.nn.Identity()\n",
    "        \n",
    "        self.mlp_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=in_features, out_features=out1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout1),\n",
    "\n",
    "            torch.nn.Linear(in_features=out1, out_features=out2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout2),\n",
    "\n",
    "            torch.nn.Linear(in_features=out2, out_features=out3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout3),\n",
    "        )\n",
    "        \n",
    "        self.classifier_head = torch.nn.Linear(in_features=out3, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_features = self.extractor(x)\n",
    "        mlp_features = self.mlp_layer(img_features)\n",
    "        prediction = self.classifier_head(mlp_features)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dc96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "class DamageCaptioningModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        yolo_location_path: str,\n",
    "        resnet_pth_path: str,\n",
    "        yolo_severity_path: str,\n",
    "        captioning_path: str,\n",
    "        resnet_transform,\n",
    "        severity_names,\n",
    "        location_names,\n",
    "        num_resnet_classes: int = 2,\n",
    "        num_severity_classes: int = 3,\n",
    "        damage_threshold: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = CONFIG['DEVICE']\n",
    "        self.damage_threshold = damage_threshold\n",
    "        self.resnet_transform = resnet_transform\n",
    "        self.severity_names = severity_names\n",
    "        self.location_names = location_names\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # RESNET — DAMAGE / NO DAMAGE\n",
    "        # --------------------------------------------------\n",
    "        checkpoint = torch.load(CONFIG['RESNET_CLASSIFIER_PATH'], map_location=CONFIG['DEVICE'])\n",
    "\n",
    "        self.resnet = Resnet50CustomModel(\n",
    "            dropout1=checkpoint['config']['dropout1'],\n",
    "            dropout2=checkpoint['config']['dropout2'],\n",
    "            dropout3=checkpoint['config']['dropout3'],\n",
    "            out1=checkpoint['config']['out1'],\n",
    "            out2=checkpoint['config']['out2'],\n",
    "            out3=checkpoint['config']['out3'],\n",
    "        )\n",
    "\n",
    "        self.resnet.load_state_dict(checkpoint['state_dict'])\n",
    "        self.resnet.to(CONFIG['DEVICE']).eval()\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # YOLO MODELS\n",
    "        # --------------------------------------------------\n",
    "        self.yolo_location = YOLO(yolo_location_path)\n",
    "        self.yolo_severity = YOLO(yolo_severity_path)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # ViT + GPT2 Captioning\n",
    "        # --------------------------------------------------\n",
    "        self.vitgpt_model = VisionEncoderDecoderModel.from_pretrained(\n",
    "            captioning_path\n",
    "        ).to(CONFIG['DEVICE']).eval()\n",
    "\n",
    "        self.vitgpt_processor = ViTImageProcessor.from_pretrained(\n",
    "            captioning_path\n",
    "        )\n",
    "\n",
    "        self.vitgpt_tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "            captioning_path\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # DAMAGE CLASSIFICATION\n",
    "    # --------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def _predict_damage(self, image: Image.Image):\n",
    "        x = self.resnet_transform(image).unsqueeze(0).to(self.device)\n",
    "        logits = self.resnet(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        damaged_prob = probs[0, 1].item()\n",
    "        is_damaged = damaged_prob > self.damage_threshold\n",
    "        return is_damaged, damaged_prob\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # YOLO HELPER\n",
    "    # --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _extract_best_detection(yolo_result):\n",
    "        if yolo_result.boxes is None or len(yolo_result.boxes) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        idx = yolo_result.boxes.conf.argmax()\n",
    "        box = yolo_result.boxes.xyxy[idx].cpu().numpy()\n",
    "        cls = int(yolo_result.boxes.cls[idx])\n",
    "        conf = float(yolo_result.boxes.conf[idx].item())\n",
    "        return box, cls, conf\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _predict_severity(self, image: Image.Image):\n",
    "        result = self.yolo_severity(image)[0]\n",
    "\n",
    "        if result.boxes is None or len(result.boxes) == 0:\n",
    "            return \"unknown\", None\n",
    "\n",
    "        idx = result.boxes.conf.argmax()\n",
    "        cls = int(result.boxes.cls[idx])\n",
    "        box = result.boxes.xyxy[idx].cpu().numpy()\n",
    "\n",
    "        severity_name = self.yolo_severity.names[cls]\n",
    "        return severity_name, box\n",
    "    \n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # FORWARD\n",
    "    # --------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def forward(self, image: Image.Image):\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        np_image = np.array(image)\n",
    "\n",
    "        # DAMAGE CLASSIFICATION\n",
    "        is_damaged, damage_prob = self._predict_damage(image)\n",
    "\n",
    "        # IMAGE CAPTIONING (ALWAYS)\n",
    "        pixel_values = self.vitgpt_processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values.to(self.device)\n",
    "\n",
    "        output_ids = self.vitgpt_model.generate(\n",
    "            pixel_values,\n",
    "            max_length=50,\n",
    "            num_beams=5\n",
    "        )\n",
    "\n",
    "        raw_caption = self.vitgpt_tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # NO DAMAGE CASE\n",
    "        if not is_damaged:\n",
    "            return {\n",
    "                \"image_pil\": image,\n",
    "                \"image_np\": np_image,\n",
    "\n",
    "                \"damaged\": False,\n",
    "                \"damage_probability\": damage_prob,\n",
    "\n",
    "                \"caption\": f\"A car with no visible damage. {raw_caption}\",\n",
    "\n",
    "                \"location\": None,\n",
    "                \"severity\": None,\n",
    "                \"location_box\": None,\n",
    "                \"severity_box\": None,\n",
    "            }\n",
    "\n",
    "        # YOLO LOCATION\n",
    "        loc_result = self.yolo_location(image)[0]\n",
    "        loc_box, loc_cls, _ = self._extract_best_detection(loc_result)\n",
    "\n",
    "        location_name = (\n",
    "            self.yolo_location.names[loc_cls]\n",
    "            if loc_cls is not None else \"unknown\"\n",
    "        )\n",
    "\n",
    "        # SEVERITY\n",
    "        severity_name, severity_box = self._predict_severity(image)\n",
    "\n",
    "        # FINAL CAPTION\n",
    "        final_caption = (\n",
    "            f\"A car with {location_name} damage of {severity_name} severity. \"\n",
    "            f\"{raw_caption}\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image_pil\": image,\n",
    "            \"image_np\": np_image,\n",
    "\n",
    "            \"damaged\": True,\n",
    "            \"damage_probability\": damage_prob,\n",
    "\n",
    "            \"location\": location_name,\n",
    "            \"severity\": severity_name,\n",
    "            \"location_box\": loc_box,\n",
    "            \"severity_box\": severity_box,\n",
    "\n",
    "            \"caption\": final_caption,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8299e80",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1834a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE YOLO CHECKPOINTS LOCALLY\n",
    "    # -------------------------\n",
    "    yolo_loc_path = os.path.join(save_dir, \"yolo_location.pt\")\n",
    "    yolo_sev_path = os.path.join(save_dir, \"yolo_severity.pt\")\n",
    "\n",
    "    shutil.copy(model.yolo_location.ckpt_path, yolo_loc_path)\n",
    "    shutil.copy(model.yolo_severity.ckpt_path, yolo_sev_path)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE CAPTION MODEL\n",
    "    # -------------------------\n",
    "    caption_dir = os.path.join(save_dir, \"caption_model\")\n",
    "    model.vitgpt_model.save_pretrained(caption_dir)\n",
    "    model.vitgpt_processor.save_pretrained(caption_dir)\n",
    "    model.vitgpt_tokenizer.save_pretrained(caption_dir)\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE RESNET (FULL CHECKPOINT)\n",
    "    # -------------------------\n",
    "    resnet_payload = {\n",
    "        \"state_dict\": model.resnet.state_dict(),\n",
    "        \"config\": {\n",
    "            \"dropout1\": model.resnet.mlp_layer[2].p,\n",
    "            \"dropout2\": model.resnet.mlp_layer[5].p,\n",
    "            \"dropout3\": model.resnet.mlp_layer[8].p,\n",
    "            \"out1\": model.resnet.mlp_layer[0].out_features,\n",
    "            \"out2\": model.resnet.mlp_layer[3].out_features,\n",
    "            \"out3\": model.resnet.mlp_layer[6].out_features,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    torch.save(\n",
    "        resnet_payload,\n",
    "        os.path.join(save_dir, \"resnet_damage.pth\")\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # SAVE META\n",
    "    # -------------------------\n",
    "    meta = {\n",
    "        \"yolo_location_ckpt\": f\"models/inference_model/yolo_location.pt\",\n",
    "        \"yolo_severity_ckpt\": f\"models/inference_model/yolo_severity.pt\",\n",
    "        \"caption_model_dir\": f\"models/inference_model/caption_model\",\n",
    "        \"damage_threshold\": model.damage_threshold,\n",
    "    }\n",
    "\n",
    "    torch.save(meta, os.path.join(save_dir, \"meta.pt\"))\n",
    "\n",
    "    print(f\"✅ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb0588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(image=None, result=None):\n",
    "    \"\"\"\n",
    "    image  : PIL.Image or None\n",
    "    result : dict returned by DamageCaptioningModel\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # IMAGE SOURCE\n",
    "    # -------------------------\n",
    "    if image is None:\n",
    "        image = result.get(\"image_pil\", None)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(\"No image provided to visualize.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # CREATE FIGURE\n",
    "    # -------------------------\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    damaged = result.get(\"damaged\", False)\n",
    "\n",
    "    # -------------------------\n",
    "    # DRAW BOX HELPER\n",
    "    # -------------------------\n",
    "    def draw(box, label, color):\n",
    "        if box is None:\n",
    "            return\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            linewidth=2,\n",
    "            edgecolor=color,\n",
    "            facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            x1,\n",
    "            max(y1 - 10, 0),\n",
    "            label,\n",
    "            color=\"white\",\n",
    "            fontsize=11,\n",
    "            weight=\"bold\",\n",
    "            bbox=dict(facecolor=color, alpha=0.7, pad=2)\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # DRAW YOLO RESULTS\n",
    "    # -------------------------\n",
    "    if damaged:\n",
    "        # LOCATION BOX\n",
    "        draw(\n",
    "            result.get(\"location_box\"),\n",
    "            f\"Location: {result.get('location', 'unknown')}\",\n",
    "            \"red\"\n",
    "        )\n",
    "\n",
    "        # SEVERITY BOX\n",
    "        draw(\n",
    "            result.get(\"severity_box\"),\n",
    "            f\"Severity: {result.get('severity', 'unknown')}\",\n",
    "            \"blue\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # TITLE & CLEANUP\n",
    "    # -------------------------\n",
    "    caption = result.get(\"caption\", \"\")\n",
    "    ax.set_title(caption, fontsize=12, wrap=True)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e88d14",
   "metadata": {},
   "source": [
    "# Loading, saving, and testing the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86b05a",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617b1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DamageCaptioningModel(\n",
    "    yolo_location_path=CONFIG['YOLO_LOCATION_PATH'],\n",
    "    resnet_pth_path=CONFIG['RESNET_CLASSIFIER_PATH'],\n",
    "    yolo_severity_path=CONFIG['YOLO_SEVERITY_PATH'],\n",
    "    captioning_path=CONFIG['CAPTIONING_MODEL'],\n",
    "    resnet_transform=img_transform,\n",
    "    severity_names=CONFIG['SEVERITY_NAMES'],\n",
    "    location_names=CONFIG['LOCATION_NAMES'],\n",
    "    damage_threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ada24",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a266140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to: ../../models/inference_model\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "save_model(model, CONFIG['MODEL_FINAL_SAVE_DIR'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Portfolio_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
